%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{cite}
\usepackage{fixltx2e}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Evaluate with Confidence Estimation: Machine ranking of translation
outputs using grammatical features}

\author{
  Eleftherios Avramidis, Maja Popovic, David Vilar, Aljoscha Burchardt \\
  German Research Center for Artificial Intelligence (DFKI) \\
  Language Technology (LT), Berlin, Germany \\
  {\tt name.surname@dfki.de} 
  %\\\And
  %Second Author \\
  %Affiliation / Address line 1 \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  %{\tt email@domain} \\
  }

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present a pilot study on an evaluation method which is able to rank 
translation outputs with no reference translation, given only their source sentence. The
system employs a statistical classifier trained upon existing human rankings, 
using several features derived from analysis of both the source and the target
sentences. Development experiments on one language pair showed that the method
has considerably good correlation with human ranking when using features
obtained from a PCFG parser.
  
\end{abstract}

\section{Introduction}
Automatic evaluation metrics for Machine Translation (MT) have mainly relied on
analyzing both the MT output against (one or more) reference translations.
Though, several paradigms in Machine Translation Research pose the need to
estimate the quality through many translation outputs, when no reference
translation is given (\textit{n}-best rescoring of SMT systems, system
combination etc.). Such metrics have been known as \textit{Confidence
Estimation metrics} and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task,
we propose using Confidence Estimation metrics for MT evaluation. This way we
allow such a metric to be directly compared with the conventional
reference-aware MT metrics.
%an effort is being made to include a confidence
%estimation metric among other conventional MT metrics for the first time, in
%order to allow its evaluation at the same level with the reference-aware
%approaches.

Our approach suggests building a Confidence Estimation metric using
already existing human judgments. This has been motivated by the existence
of human-annotated data containing comparisons of the outputs of several
systems, as a result of the evaluation tasks run by the Workshops on
Statistical Machine Translation
(\textsc{WMT})
\cite{callisonburch-EtAl:2008:WMT,callisonburch-EtAl:2009:WMT-09,callisonburch-EtAl:2010:WMT}. This amount of data, which has been freely available for further research, gives
an opportunity for applying machine learning techniques to model the human
annotators' choices. Machine Learning methods over previously released 
evaluation data have been already used for tuning complex
statistical evaluation metrics (e.g. SVM-Rank in
\cite{callisonburch-EtAl:2010:WMT}). %TODO: example example example
Our proposition is similar, but works without reference translations. 
We develop a solution of applying machine learning in order to build a
statistical classifier that performs similar to the human ranking: it is trained
to rank several MT outputs, given analysis of possible qualitative criteria on
both the source and the target side of every given sentence. As qualitative
criteria, we use statistical features indicating the quality and
the grammaticality of the output.

\section{Automatic ranking method}
\subsection{From Confidence Estimation to ranking}
Confidence estimation has been seen from the Natural Language Processing (NLP)
perspective as a problem of binary classification in order to assess the
correctness of a NLP system output. Previous work focusing on Machine
Translation includes statistical methods for estimating correctness scores or
correctness probabilities, following a rich search over the spectrum of
possible features
\cite{Blatz04confidenceestimation,Ueffing05word-levelconfidence,Specia09TWSS,raybaud2009word,Rosti07combiningoutputs}.

In this work we slightly transform the binary classification practice to fit
the standard WMT human evaluation process. As human annotators have provided
their evaluation in the form of ranking of five system outputs at a sentence
level, we build our evaluation mechanism with similar functionality, aiming to
training from and evaluating against this data. Evaluation scores and results
can be then calculated based on comparative analysis of the performance of each
system.

\subsection{Internal pairwise decomposition\label{Internal pairwise
decomposition} }
We build one classifier over all input sentences. While the evaluation
mechanism is trained and evaluated on a multi-class (ranking) basis as explained
above, the classifier is expected to work on a binary level: we provide the
features from the analysis of the two system outputs and the source, and the
classifier should decide if the first system output is better than the second one or
not. 

In order to accomplish such training, the \textit{n} systems' outputs for each
sentence are broken down to \(n \times (n-1)\) pairs, of all possible
comparisons between two system outputs, in both directions (similar to the
calculation of the Spearman correlation).
% If \( T = \{ t_1, \ldots, t_n\} \) the system outputs for one input sentence,
% and \( R = \{ r_1, \ldots, r_n\} \) their respective ranks assigned by the human
% evaluators, then the pairs would be
% 
% \[ \forall pair(t_i,t_j) where t_i, t_j \in T and t_i \neq t_j \]  
For each pair, the classifier is trained with a class value \(c\), for the
pairwise comparison of system outputs $t_i$ and $t_j$ with respective ranks
\(r_i\) and \(r_j\), determined as:

\[
c(r_i, r_j) = \left\{
  \begin{array}{l l}
    1 & \quad r_i < r_j \\
    -1 & \quad r_i > r_j \\
  \end{array} \right.
\] 

At testing time, after the classifier has made all the pairwise decisions, those
need to be converted back to ranks. System entries are ordered, according to how
many times each of them won in the pairwise comparison, leading to rank lists
similar to the ones provided by the humans. Note that this kind of
decomposition allows for \textit{ties} when there are equal times of winnings. %Though, ties are ignored during the evaluation process (when they
%occur) so as to fit the evaluation standards of the shared task.

\begin{table*}[h!]
\begin{center}
\begin{tabular}{|l|rl|rl|}
\hline
features & \multicolumn{2}{c|}{ naive Bayes } &  \multicolumn{2}{c|}{ knn }  \\
\hline
& rho & tau & rho & tau \\
\hline
ngram & 0.19 & 0.05 & 0.13 & 0.01  \\
unk,len & 0.67 & 0.20 & 0.73 & 0.24  \\
unk,len, bigram & 0.61 & 0.21 & 0.74 & 0.21 \\
unk,len, ngram & 0.63 & 0.19 & 0.59 & 0.21  \\
unk,len, trigram & 0.67 & 0.20 & 0.76 & 0.21  \\
unk,len, log_{parse} & 0.75 & 0.21 & 0.74 & 0.25 \\
\hline
unk,len, n_parse,VP & 0.67 & 0.24 & 0.61 & 0.20  \\
unk,len, n_parse,VP,conf_{bestparse} & 0.72 & 0.20 & 0.80 & 0.25  \\
unk,len, n_parse,NP,conf_{bestparse} & 0.75 & 0.21 & 0.78 & 0.23  \\
\hline
unk,len, n_parse,VP,conf_{avg} & 0.75 & 0.21 & 0.78 & 0.23 \\
unk,len, n_parse,VP,conf_{bestparse} & 0.78 & 0.25 & 0.75 & 0.24 \\
unk,len, n_parse,VP,log_{parse} & \textbf{0.81} & \textbf{0.26} & 0.75 & 0.23  \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Spearman rho and Kendall tau correlation
coefficients achieved on automatic ranking}
\end{table*}

\subsection{Acquiring features\label{features}}
In order to obtain features indicating the quality of the MT output,
automatic NLP analysis tools are applied on both the source and the two target
(MT-generated) sentences of every pairwise comparison. Features
considered can be seen in the following categories, according to their origin:

\begin{itemize}
  \item \textbf{Sentence length:} Number of words of source and target
  sentences, source-length / target-length ratio. 
  \item \textbf{Target language model:} Language models provide statistics
  concerning the correctness of the words' sequence on the target language. Such
  language model features include:
   \begin{itemize} 
     \item the smoothed \textit{n}-gram probability of   the entire target
     sentence for a language model of order 5, along with
     \item uni-gram, bi-gram, tri-gram probabilities and a 
     \item count of unknown words
    \end{itemize}
  \item \textbf{Parsing:} Processing features acquired from PCFG
  parsing \cite{Petrov06learningaccurate} for both source and target side
  include:
  \begin{itemize}
    \item parse log likelihood, 
    \item number of n-best trees, 
    \item confidence for the best parse,
    \item average confidence of all trees.
   \end{itemize}
    Ratios of the above target features with their respective source features
    were included.
  \item \textbf{Shallow grammatical match:} The number of occurences of
  particular node tags on both the source and the target was counted on
  the PCFG parses. In particular, NPs, VPs, PPs, NNs and punctuation occurences
  were counted. Then the ratio of the occurences of each tag in the target
  sentence by its occurences on the source sentence was also calculated.
\end{itemize}

\subsection{Classifiers}
The machine learning core of the system was built supporting two
classification approaches. 
\begin{itemize}
  \item \textbf{Naive Bayes} allows prediction of a binary class, given the
  assumption that the features are statistically independent. 
  \[
  p(C, F_1, \ldots, F_n) = p(C)\prod_{n}^{i=1}p(F_i|C)
  \]
  
  \(p(C)\) is estimated by relative frequencies of the training pairwise
  examples, while \(p(F_i|C)\) for our continuous features are
  estimated with LOESS (locally weighted linear regression similar to
  \cite{cleveland1979robust})
  \item \textbf{k-nearest neighbour} (knn) algorithm allows classifying based
  on the closest training examples in the feature space. 
\end{itemize}





\section{Experiment}
\subsection{Experiment setup}
The classifiers for the task were learnt using the German-English testset
 of the \textsc{WMT} 2008 and 2010 (about 700
 sentences)\footnote{data acquired from http://www.statmt.org/wmt11}. For
 testing, the classifiers were used to perform ranking on a test set of 184
 sentences which had been kept apart from the 2010 data, with the criterion that
 they don't contain contradictions among the human judgments.
 
 Tokenization was performed with the \textsc{PUNKT} tokenizer
 \cite{Kiss_2006unsupervised,Garrette_anextensible}, while n-gram features were
 generated with the \textsc{SRILM} toolkit \cite{Stolcke02srilm}.
 The language model was relatively big and had been built upon all lowercased
 monolingual training sets for the WMT 2011 Shared Task, interpolated on the
 2007 test set. As a PCFG parser, the Berkeley Parser
 \cite{Petrov07improvedinference} was preferred, due to the possibility of
 easily obtaining complex internal statistics, including \textit{n}-best trees.
 Unfortunately, the time required for parsing leads to significant delays at
 the overall processing. The machine learning algorithms were implemented with the Orange
 toolkit \cite{demsar2004orange}.

\subsection{Feature Selection}
Although the automatic NLP tools provided a
lot of features (section \ref{features}), the classifier methods we used, would
be expected to perform better given a smaller group of statistically independent
features. Since exhaustive training/testing of all possibile feature subsets 
was not possible, we performed feature selection based on the Relieff method
\cite{Kononenko94estimatingattributes:,kira1992feature}. Automatic Ranking was
performed based on the most promising feature subsets. The results are examined below.

\subsection{Results}
The performance of the classifier is measured after the classifier
output has been converted back to rank lists, similar to the WMT 2010
evaluation. We therefore calculated two types of rank coefficients: averaged
Kendall's tau on a segment level, and Spearman's rho on a system level, based
on the percentage that the each system's translations performed better than or equal to the translations
of any other system.

The results for the various combinations of features and classifiers are
depicted on Table 1. Naive Bayes provides the best score on the test set, with
$\rho = 0.81$ on a system level and $\tau = 0.26$, trained with features
including the number of the unknown words, the source-length by target-length
ratio, the VP count ratio and the source-target ratio of the parsing
log-likelihood. The number of unknown words particularly appears to be a strong indicator for the quality of
the sentence. On the first part of the table we can also observe that language
model features do not perform as well as the features deriving from the
processing information delivered by the parser. On the second part of the table
we compare the use of various grammatical combinations. Finally, the last part
contains the correlation obtained by various similar internal parsing-related features.

\section {Conclusion and Further work}
The experiment presented in this article indicates that confidence metrics
trained over human rankings can be possibly used for several tasks of
evaluation, given particular conditions, where e.g. there is no reference
translation given. Features obtained from a PCFG parser seem to be
leading to good correlations, given our test set. 

Nevertheless this is still a small-scale experiment, given the restricted data
size and the single translation direction. The performance of the system on
broader training and test sets will be evaluated in the future. Feature
selection is also expected to change if other language pairs are introduced,
while more sophisticated machine learning algorithms may also leed to better
results. 

\section*{Acknowledgments}
This work was done with the support of the TaraX\H{U } Project, financed by TSB 
Technologiestiftung Berlin--Zukunftsfonds Berlin, co-financed by the European 
Union--European fund for regional development.

\bibliographystyle{acl} 
\bibliography{mybib} 

% \begin{thebibliography}{}
%  
% \bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
% Alfred~V. Aho and Jeffrey~D. Ullman.
% \newblock 1972.
% \newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
% \newblock Prentice-{Hall}, Englewood Cliffs, NJ.
% 
% \bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
% {American Psychological Association}.
% \newblock 1983.
% \newblock {\em Publications Manual}.
% \newblock American Psychological Association, Washington, DC.
% 
% \bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
% {Association for Computing Machinery}.
% \newblock 1983.
% \newblock {\em Computing Reviews}, 24(11):503--512.
% 
% \bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
% Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
% \newblock 1981.
% \newblock Alternation.
% \newblock {\em Journal of the Association for Computing Machinery},
%   28(1):114--133.
% 
% \bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
% Dan Gusfield.
% \newblock 1997.
% \newblock {\em Algorithms on Strings, Trees and Sequences}.
% \newblock Cambridge University Press, Cambridge, UK.
% 
% \end{thebibliography}

\end{document}
