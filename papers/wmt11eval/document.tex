%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{cite}
\usepackage{fixltx2e}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Evaluate with Confidence Estimation: Machine ranking of translation
output using grammatical features}

\author{
  Names \\
  German Research Center for Artificial Intelligence \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  {\tt emails@dfki.de} 
  %\\\And
  %Second Author \\
  %Affiliation / Address line 1 \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  %{\tt email@domain} \\
  }

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present an evaluation method which is able to rank multiple translation
outputs with no reference translation, given only their source sentence. The
system employs a statistical classifier trained upon existing human rankings, 
using several features deriving from analysis of both the source and the target sentences. Experiments  on
one language pair of the evaluation with various feature subsets showed 
that the method has considerably good correlation to the human ranking 
when using features obtained from a PCFG parser.
  
\end{abstract}

\section{Introduction}
Automatic evaluation metrics of Machine Translation have mainly relied on
analyzing both the MT output against (one or more) reference translations.
Though, several paradigms in Machine Translation (MT) Research pose the need to
estimate the quality through many translation outputs, when no reference translation is given (\textit{n}-best rescoring of SMT
systems, system combination etc.). Such metrics have been
known as \textit{confidence estimation metrics} and quite a few projects have
suggested solutions on this direction. %TODO: add here summary from previous efforts
With this paper, an effort is being made to align confidence estimation metrics
with conventional MT metrics and allow further comparison.

Additionally, human evaluation tasks run by Workshops on Machine
Translation provide a big amount of human-annotated data
with comparisons of the outputs of several systems. This amount of data,
which has been freely available for further research, gives an opportunity for
applying machine learning techniques an effort to model the
human annotators' choices. Machine Learning methods over previously released evaluation data
have been already used for tuning complex statistical evaluation metrics in the
past. %TODO: example example example
Here, we propose a solution of applying machine learning in order to build a
classifier able to perform similar to the human ranking, by
ranking several MT outputs, given analysis of possible qualitative criteria on
both the source and the target side of every given sentence. In particular we
examine the use of statistical features indicating the quality and the
grammaticality of the output.

\section{Automatic ranking method}
\subsection{From Confidence Estimation to ranking}
Confidence estimation has been seen from the Natural Language Processing (NLP)
perspective as a problem of binary classification in order to assess the
correctness of an NLP's system output. Previous work focusing on Machine
Translation includes statistical methods for estimating correctness scores or
correctness probabilities, following a rich search over the spectrum of
possible features
\cite{Blatz04confidenceestimation,Ueffing05word-levelconfidence,Specia09TWSS,raybaud2009word,Rosti07combiningoutputs}.

In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our system with similar functionality, aiming to training from and evaluating against this data. Evaluation scores and results
can be then calculated based on comparative analysis of the performance
of each system.

\subsection{Internal pairwise decomposition\label{Internal pairwise
decomposition} }
While the system is trained and evaluated on a multi-class basis, as explained
above, classifiers are expected to work in a binary level. The \textit{n}
systems output for each sentence is therefore broken down to \(n \times n\)
pairs, of all possible comparisons between two systems, on both directions (similar to the calculation of the Spearman correlation). After the classifier has made all
the pairwise decisions, system entries are ordered based on how many times each
of them won in the pairwise comparison. Note that this kind
of decomposition allows for \textit{ties}, when there are equal times of
winnings. Though, ties are ignored during the evaluation process (when they
occur) so as to fit the evaluation standards of the shared task.

\subsection{Feature selection}
In order to obtain features indicating the quality of the MT output, analysis on
both the source and the two target (MT-generated) sentences is performed. Features
considered can be seen in the following categories, according to their origin:

\begin{itemize}
  \item \textbf{Sentence length:} Count of tokens for source and target
  sentences, source/target length ratio. 
  \item \textbf{Language model:} The smoothed \textit{n}-gram
  probability of the entire target sentence for a language model of order 5,
  along with bi-gram and tri-gram probabilities and a count of unknown words
  \item \textbf{Parsing:} Processing features acquired from PCFG
  parsing~\cite{Petrov06learningaccurate} for both source and side included
  parse log likelihood, number of n-best trees, confidence for best parse,
  average confidence of all trees. Source/target ratios were included.
  \item \textbf{Shallow grammatical match:} We counted the occurences of
  particular nodes tags on both the source and the target and calculated their ratio. In
  particular, we counted NPs, VP, PPs, NNs and punctuation occurences.
\end{itemize}

\subsection{Classifiers}
The machine learning core of the system was built supporting two
classification approaches. 
\begin{itemize}
  \item \textbf{Naive Bayes} allows prediction of a binary class, given the
  assumption that the features are statistically independent. 
  \[
  p(C, F_1, \ldots, F_n) = \frac{1}{Z}p(C)\prod_{n}^{i=1}p(F_i|C)
  \]
  
  \(p(C)\) is estimated by relative frequencies of the training pairwise
  examples, while \(p(F_i|C)\) for our continuous features are
  estimated with LOESS (locally weighted linear regression similar to
  \cite{cleveland1979robust})
  \item \textbf{k-nearest neighbour} (knn) algorithm allows classifying based
  on the closest training examples in the feature space. 
\end{itemize}

\section{Experiment}
\subsection{Experiment setup}
The classifiers for the task were learnt using the German-English
 of the \textsc{WMT} 2008 and 2010 testset (about 700 sentences). For testing,
 the classifiers were used to perform ranking on a test set of 184 sentences
 which was kept apart from the 2010 data so that it doesn't contain any ties
 among the human judgments. Ranking was perform wrapped into pre- and
 postprocessing mechanism to allow internal pairwise classification
 (as explained in \ref{Internal pairwise decomposition}).
 
 Tokenization was performed with the \textsc{PUNKT} tokenizer \cite{Kiss_2006unsupervised,Garrette_anextensible}, while n-gram features were generated with the
 \textsc{SRILM} toolkit~\cite{Stolcke02srilm—anextensible}. The language model
 was relatively big and had been trained on lowercased monolingual training sets
 for the \textsc{WMT11} Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley
 Parser \cite{Petrov07improvedinference} was preferred, due to the possibility
 of easily obtaining complex internal statistics, including \textit{n}-best
 trees. The machine learning algorithms were implemented with the Orange toolkit
 \cite{demšar2004orange}.
   
 
 
\begin{table*}[h]
\begin{center}
\begin{tabular}{|l|rl|rl|}
\hline
features & \multicolumn{2}{c|}{ naive Bayes } &  \multicolumn{2}{c|}{ knn }  \\
\hline
& rho & tau & rho & tau \\
\hline
ngram & 0.19 & 0.05 & 0.13 & 0.01  \\
unk,len & 0.67 & 0.20 & 0.73 & 0.24  \\
unk,len, bigram & 0.61 & 0.21 & 0.74 & 0.21 \\
unk,len, ngram & 0.63 & 0.19 & 0.59 & 0.21  \\
unk,len, trigram & 0.67 & 0.20 & 0.76 & 0.21  \\
unk,len, log_{parse} & 0.75 & 0.21 & 0.74 & 0.25 \\
\hline
unk,len, n_parse,VP & 0.67 & 0.24 & 0.61 & 0.20  \\
unk,len, n_parse,VP,conf_{bestparse} & 0.72 & 0.20 & 0.80 & 0.25  \\
unk,len, n_parse,NP,conf_{bestparse} & 0.75 & 0.21 & 0.78 & 0.23  \\
\hline
unk,len, n_parse,VP,conf_{avg} & 0.75 & 0.21 & 0.78 & 0.23 \\
unk,len, n_parse,VP,conf_{bestparse} & 0.78 & 0.25 & 0.75 & 0.24 \\
unk,len, n_parse,VP,log_{parse} & \textbf{0.81} & \textbf{0.26} & 0.75 & 0.23  \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Spearman rho and kendal tau correlation coefficients
achieved on automatic ranking }
\end{table*}

\subsection{Feature Selection}
Although the analysis provided a lot of features, the classifiers that were
available for testing would be expected to perform better given a small group of
statistically independent features. Since exhaustive training/testing of all
possibile feature subsets was not possible, we performed feature selection
based on the Relieff method
\cite{Kononenko94estimatingattributes:,kira1992feature}. Automatic Ranking was
performed based on the most promising feature subsets. The results are
examined below.

\subsection{Results}
Statistical evaluation is performed on a low level after the classifier output
has been converted to ranks, following the \textsc{WMT10} evaluation. We
calculated two types of rank coefficients: averaged Kendall's tau on a segment
level, and Spearman's rho on a system level, based on the percentage that the
each system's translations performed better than or equal to the translations
of any other system.

The results for the various combinations of features and classifiers are
depicted on Table 1. Naive Bayes provides the best score on the test set, with
$\rho = 0.81$ on a system level and $\tau = 0.26$, trained with features
including the number of the unknown words, the length source/target radio, the
VP count ratio and the parsing log likelihood source/target ratio. The number of
unknown words particularly appears to be a strong indicator for the quality of
the sentence. On the first part of the table we can also observe that language
model features don't perform as well as the features deriving from the
processing information delivered by the parser. On the second part of the table
we compare the use of various grammatical combinations. Finally, the last part
of contains the correlation obtained by various similar internal
parsing-relaterd features.



\section {Conclusion and Further work}
The experiment presented in this article indicates that confidence metrics
trained over human rankings can be possibly used for several tasks of
evaluation, given particular conditions, where e.g. there is no reference
translation given. Meanwhile, features obtained from a PCFG parser seem to be
leading to good correlations, given our test set. 

Nevertheless this is still a small-scale experiment, given the restricted data
size and the single language direction. The performance of the system on broader
training and test sets will be evaluated in the future. Features
selection is also expected to change if other language pairs are introduced,
while more sophisticated machine learning algorithms may also leed to better
results. 

\section*{Acknowledgments}
This work was done with the support of the TaraXÜ Project, financed by TSB 
Technologiestiftung Berlin--Zukunftsfonds Berlin, co-financed by the European 
Union--European fund for regional development.

\bibliographystyle{acl} 
\bibliography{mybib} 

% \begin{thebibliography}{}
%  
% \bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
% Alfred~V. Aho and Jeffrey~D. Ullman.
% \newblock 1972.
% \newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
% \newblock Prentice-{Hall}, Englewood Cliffs, NJ.
% 
% \bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
% {American Psychological Association}.
% \newblock 1983.
% \newblock {\em Publications Manual}.
% \newblock American Psychological Association, Washington, DC.
% 
% \bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
% {Association for Computing Machinery}.
% \newblock 1983.
% \newblock {\em Computing Reviews}, 24(11):503--512.
% 
% \bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
% Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
% \newblock 1981.
% \newblock Alternation.
% \newblock {\em Journal of the Association for Computing Machinery},
%   28(1):114--133.
% 
% \bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
% Dan Gusfield.
% \newblock 1997.
% \newblock {\em Algorithms on Strings, Trees and Sequences}.
% \newblock Cambridge University Press, Cambridge, UK.
% 
% \end{thebibliography}

\end{document}
